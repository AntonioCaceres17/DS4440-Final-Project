<!doctype html>
<html lang="en">
<head>
<title>DiffusionZeroShotClassifier</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Diffusion Zero-Shot Classification</nobr>
 <nobr class="widenobr">For DS4440</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of <i> Your Diffusion Model is Secretly a Zero-Shot Classifier</i></h2>
<p>The remarkable discovery in the study "Your Diffusion Model is Secretly a Zero-Shot Classifier" that diffusion models are inherently capable of zero-shot classification excites us and makes us wonder about the underlying mechanisms of how these models identify and produce complex data patterns.
</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Team Members</h2>                                              

<p> Filip Tomovski
  |<a href="malito:tomovski.f@northeastern.edu">tomovski.f@northeastern.edu</a>
  | <a href=https://github.com/ftomovski>Github</a>
</p>
<p>
  Antonio Caceres 
  | <a href="malito:caceres.an@northeastern.edu">caceres.an@northeastern.edu</a>
  | <a href=https://github.com/AntonioCaceres17>Github</a>
</p>
 
<h2> Section 1: Introduction</h2>
<p><b>Project Overview and Purpose:</b>
<br>
The purpose of this study is to analyze and build upon the interesting results reported in Alexander C. Li et al. "Your Diffusion Model is Secretly a Zero-Shot Classifier". Although the main result of this research is that diffusion models can handle zero-shot classification problems using class-conditional density estimates; our effort seeks to explore the flexibility and effectiveness of this method further. We investigate the following main question: Can diffusion models' zero-shot classification capacity be successfully used to a larger collection of data and more diverse tasks than what was first tested?
<br>
<br>
<b>Defining the Problem:</b>
<br>
The ability to predict with algorithms has been greatly improved by zero-shot learning (ZSL), especially in situations when labeled data is limited or the model has to adjust to new, unknown categories. In this research, the classification of pictures without prior exposure to particular class labels during training is achieved by using diffusion models, which are usually employed for image generation. Inspired by this, our group aims to:
<br>
<br>
Expand the use of Diffusion Classifier to new datasets: We are going to evaluate the model's performance on datasets that are very different from those used in the first work, with an emphasis on datasets with more complexity and class variation.
<br>
<br>
Maximise and modify the approach: We want to improve the model's accuracy and efficiency, thereby making it more appropriate for real-world applications, by modifying and improving the model architecture and the training procedure described in the study.
<br>
<br>
Using visualization tools: we want to explain how the model makes decisions beyond numerical benchmarks. This will include looking at how various classes affect the generation process and visualizing class-conditional generation pathways.
<br>
<br>
<b>Project Significance:</b>
<br>
By pushing the boundaries of what generative models can achieve, especially in the field of zero-shot learning, this research will further our knowledge of multimodal AI systems. It will evaluate if the generative modeling principles could be viable, if not better, substitutes for the traditional discriminative methods employed in machine learning for classification problems.
<br>
<br>
<b>Expected Outcomes:</b>
<br>
This project is expected to have multiple outcomes:
<br>
Demonstration of Broader Applicability: The Diffusion Classifier's durability and adaptability in managing a variety of challenging situations would be demonstrated by successfully deploying it to additional datasets.
<br>
<br>
Methodological Advancements: Enhancements in the structure or training schedule of the model could shed light on more economical approaches to use generative models for classification problems.
<br>
<br>
Deeper Knowledge of Model Dynamics: We anticipate learning more about the decision-making processes of diffusion models, especially how they balance the generating and classifying duties.
</p>

<h2> Section 2: Review of the Paper</h2>
<p> <b>Summary and Explanation:</b>
<br>
In their study "Your Diffusion Model is Secretly a Zero-Shot Classifier," Alexander C. Li et al. explore a novel use of diffusion models that extends their application to zero-shot classification challenges without further training outside of the generating domain. Diffusion models are well-known for their capacity to produce excellent pictures by simulating a data distribution by adding and eliminating noise. By using the conditional density estimates they offer, their research reveals their promise in classification challenges, a unique method of using generative models for discriminative tasks.
<br>
<br>
Leveraging the class-conditional generating capabilities of these models, the authors provide an innovative approach known as the Diffusion Classifier. Effectively using the reconstructed data fidelity as a proxy for class likelihood, the model evaluates the probability of the input data under each class label and conditions the diffusion process on those class labels. With this technique, a generative process becomes a discriminative task that enables the model to do zero-shot classification by evaluating which class condition best recovers the original input from its noisy state.
<br>
<br>
<b>Main Takeaways:</b>
<br>
Versatility of Diffusion Models: This work shows how diffusion models may be used in real-world AI problems as they can not only generate pictures but also identify them with great accuracy.
<br>
<br>
Diffusion models' inherent capabilities are used to enable zero-shot learning, in which the model uses solely class-conditional generative processes to categorize pictures into categories it has not seen during training.
<br>
<br>
Comparative Advantage: The Diffusion Classifier performs remarkably well on a number of tests, especially in compositional reasoning tasks where it outperforms conventional discriminative and generative models. This suggests its great promise in jobs needing in-depth semantic comprehension.
<br>
<br>
<b>Relation to Our Project:</b>
<br>
Our effort is centered on this publication as we want to duplicate and expand the capabilities of the diffusion classifier. Our study tests the model on fresh datasets and modifies the methodology to maximize performance, in addition to exploring the limits of zero-shot learning using generative models, by employing this novel classification approach. Diffusion models' capacity to do tasks customarily performed by discriminative models may completely change the way people view and use these models, opening the door for reliable, multipurpose AI systems.</p>

<h2> Section 3: Technical Structure</h2>
<p>
We use the generating power of diffusion models to the zero-shot categorization problem. We are able to produce images based on verbal prompts that match to CIFAR10 class labels and compare them to real photographs for classification by using the pre-trained Stable Diffusion model.
<br><br>
<b>Methodology</b>
 <br>
The subsequent phases make up the technological framework of our implementation:
<br><br>
<b>Model Selection:</b> We choose the diffusers library's StableDiffusionPipeline, which is well-known for producing realistic visuals. Because the model was pre-trained on a varied dataset, zero-shot learning requires that it be able to comprehend a broad range of ideas and objects.
<br><br>
 <b>Pre-processing:</b> The diffusion model predicts an input distribution that is matched by normalizing CIFAR10 pictures. Correct operation of the picture generating procedure that follows depends on this normalization.
<br><br>
 <b>Image Generation:</b> We produce an image from a class-related verbal prompt for every class label in CIFAR10. These suggestions help the model to create a picture that best captures the traits of the class.
<br><br>
<b> Image Comparison:</b> We employ Mean Squared Error (MSE) as a measure of similarity between the CIFAR10 images and the generated images. For each input image, the predicted class is the one with the lowest MSE loss between the produced and the real image.
<br><br>
 <b>Device Management:</b> The generating and comparing operations are much accelerated by the calculations being done on a GPU when one is available. This enables the hard jobs of picture synthesis and analysis to be handled by using the parallel processing capabilities of contemporary GPUs.
<br><br>
 <b>Implementation Details</b>
<br>
 <b>Data loading:</b> The torchvision library is used to load CIFAR10 pictures as it offers machine learning models easily standardized datasets.
<br>
 <b>Transformations:</b> The photos are transformed using a combination of ToTensor and Normalize to make sure they work with the anticipated input format of the model.
<br><br>
 <b>Model Invocation:</b> An image output is produced by the StableDiffusionPipeline, which asks for a textual prompt.
<br>
 Performance Aspects to Remember The pipeline is built with performance in mind throughout. Batch-able operations are combined to reduce GPU context switching cost, and CPU-GPU memory transfers are minimized to prevent bottlenecks.
<br><br>
 <b>Code Availability</b>
 <br>
 All of the code for our implementation—data processing, model interaction, and classification logic—is available on GitHub at this link:
 <br>
<a href="https://github.com/AntonioCaceres17/DiffusionZeroShotClassifier/blob/main/final_project.ipynb">Python Notebook</a>
 <br>
 Transparency and repeatability of our findings are ensured by the scripts and instructions included in this repository to duplicate our study.
 
</p>

<h2> Section 4: Experimental Findings</h2>
 <br>
<p> 
Here we report the experimental results of our diffusion model-based zero-shot categorization. We investigate the distribution of the predictions made by our model across several classes and evaluate the performance of our approach against a baseline model using a set of visualizations.
<br><br>
<b>Model Accuracy Comparison</b>
<br>
The 10 epoch accuracy trends of our diffusion model-based classifier against a baseline model are shown in the line chart above. It is shown that the diffusion model continuously outperforms the baseline, exhibiting increased stability across succeeding epochs in addition to superior accuracy. Higher performance variance in the baseline model might be a sign of its sensitivity to the training data or of its less generalization potential than our approach.
<br>
 <img src="Model_Accuracy" alt="Trulli" width="500" height="333">
 <br>
<b>Prediction Distribution Across Classes</b>
<br>
The class prediction frequencies of our model are shown by the bar chart. We find that there is no appreciable bias in any one class and that predictions are distributed somewhat uniformly among them. A little greater frequency of some classes, such "frog" and "truck," would call for more research on the class representations in the training dataset.
<br>
 <img src="Prediction_Classifier" alt="Trulli" width="500" height="333">
 <br>
<b>Confusion Matrix</b>
 <br>
A comprehensive picture of the classifier's performance over all classes is provided by the confusion matrix. It emphasizes that whereas classes like "ship" and "horse" are categorized rather precisely, classes like "cat" and "dog" are sometimes misclassified as the same thing. This could imply that the model finds it difficult to classify classes with visually similar characteristics, in which case increasing the variety or amount of training data in these domains might be helpful.
<br>
 <img src="Confusion_Matrix" alt="Trulli" width="500" height="333"><br>
<b>Discussion</b>
 <br>
The results of the experiments confirm that a diffusion model works well for zero-shot categorization problems. The ability of our technique to continuously maintain high accuracy implies that it has learnt strong feature representations that are less susceptible to changes in the input data. The prediction distribution validates the lack of excessive bias in our model against particular classes. But the confusion matrix offers useful information about where the model's classification skills may be strengthened, especially in identifying classes with comparable characteristics.
<br><br>
<b>Implications</b>
 <br>
The diffusion model's correctness and stability suggest it to be appropriate for practical applications where dependability and robustness are essential. Even more dependable performance may be ensured by additional model improvement guided by the insights obtained from the confusion matrix. Furthermore encouraging for jobs demanding fair treatment of several classes is the model's uniform class prediction distribution.
<br><br>
<b>Future Work</b>
 <br>
Future research might look at methods to reduce misunderstanding between certain classes, maybe by using augmentation techniques or focused data collecting. Furthermore validating our approach's adaptability and scalability would be expanding it to larger classes and more complicated datasets.
<br>
</p>


 
<h2> Section 5: Conclusions</h2>
<p>
Our work started with investigating the zero-shot classification potential of diffusion models, which are often praised for their generating power. Building on the ground-breaking discoveries in "Your Diffusion Model is Secretly a Zero-Shot Classifier," we looked at how well these models classified data that was not visible. By means of our trials, we have reached a convergence of promising outcomes and interesting directions for further study.
<br>
<b>Conclusions</b>
 <br><br>
We have obtained encouraging results when we use our diffusion model-based method for zero-shot classification on the CIFAR10 dataset. Our results support the idea that diffusion models may, in fact, be useful classifiers that go beyond the production of images. The model showed remarkable zero-shot learning skills by being able to distinguish between classes it had not been specifically trained to identify. As our visualizations show, the model's flexibility is highlighted by the consistency in performance across different classes.
<br><br>
We demonstrate with our study the promise of the diffusion model as a versatile machine learning technique that can accurately synthesise and analyze visual input. The model's performance in classification tasks implies an innate knowledge of the data characteristics necessary for classifying and creating distinct classes.
<br><br>
<b>Implications</b>
 <br><br>
The discipline will benefit much from diffusion models' capacity to operate without requiring further training or fine-tuning on particular categorization tasks. It suggests a move away from the computational burden of developing specialized classifiers from scratch and toward more effective usage of pre-trained models. Moreover, the generality shown by these models could improve our approach to issues in fields with less labeled data.
<br><br>
<b>Future Work</b>
 <br><br>
Though our findings are strong, they also present a number of fresh problems and directions for future study:
<br><br>
<b>Diverse Datasets:</b> Our method may be further tested for diffusion model durability in zero-shot classification tasks by applying it to a larger variety of datasets, particularly those with higher complexity and larger class imbalances.
<br>
 <b>Model Interpretability:</b> By examining how the diffusion models make decisions, maybe using methods like feature visualization and attribution, one might get understanding of the underlying representations that support their classification skills.
<br>
 <b>Architectural Innovations:</b> Further performance gains are promised by investigating architectural modifications and training methods that might increase the accuracy and efficiency of the model.
<br>
 <b>Cross-domain Applications:</b> Thinking about how diffusion models may be used in multimodal learning situations with text or audio as two examples of modalities.
<br>
 <br>
 To sum up, our work has not only demonstrated the adaptability of diffusion models but also mapped out new ground for their application. We foresee additional research that increases the possibilities of these effective models.
 
</p>


<h3>References</h3>

<p><a name="Diffusion">[1]</a> <a href="https://arxiv.org/pdf/2303.16203.pdf"
  >L&eacute;Alexander C. Li et al.
  <em> Your Diffusion Model is Secretly a Zero-Shot Classifier </em></a>
</p>



  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://DS4440.baulab.info/">About DS4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
